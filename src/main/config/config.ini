[cache]
# tweepy token
BEARER_TOKEN=/data/tweepy_token/BEARER_TOKEN.json
# raw data
domain_entity_pairs=/data/evergreen-context-entities-20220601.csv
domain_id=47
raw_data=/data/raw
# cleaned data
all_data_dl=/data/cleaned/all_data_dl.csv
X_train_all=/data/cleaned/X_train_200.npy
X_val_all=/data/cleaned/X_val_200.npy
X_test_all=/data/cleaned/X_test_200.npy
y_train_all=/data/cleaned/y_train_200.npy
y_val_all=/data/cleaned/y_val_200.npy
y_test_all=/data/cleaned/y_test_200.npy
X_train_50=/data/cleaned/X_train_50.npy
X_val_50=/data/cleaned/X_val_50.npy
X_test_50=/data/cleaned/X_test_50.npy
y_train_50=/data/cleaned/y_train_50.npy
y_val_50=/data/cleaned/y_val_50.npy
y_test_50=/data/cleaned/y_test_50.npy
X_train_50_non_dl = /data/cleaned/X_train_50_non_dl.npy
X_val_50_non_dl = /data/cleaned/X_val_50_non_dl.npy
X_test_50_non_dl = /data/cleaned/X_test_50_non_dl.npy
y_train_50_non_dl = /data/cleaned/y_train_50_non_dl.npy
y_val_50_non_dl = /data/cleaned/y_val_50_non_dl.npy
y_test_50_non_dl = /data/cleaned/y_test_50_non_dl.npy
hashtags_map=/data/cleaned/hashtags_map.json
hashtags_repository=/data/cleaned/hashtags_repository.csv
# model ckp
model_bert=/data/model_checkpoint/bert/bert_50tag_idfwt_200len_{}epoch.pt
model_lstm=/data/model_checkpoint/lstm/lstm_50tag_idfwt_200len_50hidden_{}epoch.pt
model_resnet=/data/model_checkpoint/resnet/resnet_50tag_idfwt_200len_4res_{}epoch.pt
# validation metrics with complex trainer
training_metrics_bert=/data/training/bert_50tag_idfwt_200len_training.json
training_metrics_lstm=/data/training/lstm_50tag_idfwt_200len_50hidden_training.json
training_metrics_resnet=/data/training/resnet_50tag_idfwt_200len_4res_training.json
# pred results
pred_proba_bert=/data/predicted_result/bert/bert_50tag_pred_proba.npy
pred_proba_lstm=/data/predicted_result/lstm/lstm_50tag_pred_proba.npy
pred_proba_resnet=/data/predicted_result/resnet/resnet_50tag_pred_proba.npy
pred_top_bert=/data/predicted_result/bert/bert_50tag_top{}.json
pred_top_lstm=/data/predicted_result/lstm/lstm_50tag_top{}.json
pred_top_resnet=/data/predicted_result/resnet/resnet_50tag_top{}.json
# evaluation and metrics
gt_val_50=/data/evaluation/gt_val_50.json
gt_test_50=/data/evaluation/gt_test_50.json
gt_stats=/data/evaluation/gt_stats.json
metrics_bert=/data/evaluation/bert/diff_epochs_metrics_bert.json
metrics_lstm=/data/evaluation/lstm/diff_epochs_metrics_lstm.json
metrics_resnet=/data/evaluation/resnet/diff_epochs_metrics_resnet.json
stats_pred_bert=/data/evaluation/bert/stats_pred_bert.json
stats_pred_lstm=/data/evaluation/lstm/stats_pred_lstm.json
stats_pred_resnet=/data/evaluation/resnet/stats_pred_resnet.json
stats_metrics_bert=/data/evaluation/bert/stats_metrics_bert.csv
stats_metrics_lstm=/data/evaluation/lstm/stats_metrics_lstm.csv
stats_metrics_resnet=/data/evaluation/resnet/stats_metrics_resnet.csv
stats_ci_bert=/data/evaluation/bert/stats_ci_bert.csv
stats_ci_lstm=/data/evaluation/lstm/stats_ci_lstm.csv
stats_ci_resnet=/data/evaluation/resnet/stats_ci_resnet.csv


[train]
device=cpu
num_epoch=30
max_len=200
batch_size=1024
threshold=0.3


[evaluate]
n_sample=50
top_n=3,5,10
epoch_list=10,20,30

[BERT]
pre_trained_model=bert-base-cased


[LSTM]
hidden_size=50


[RESNET]
num_residuals=2,2,2,2
num_channels=64,64,128,256,512
embed_size=100
